{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explored the scenario where we used one parameter $\\theta_1$ and plotted its cost function to implement a gradient descent. Our formula for a single parameter was:\n",
    "\n",
    "![gda_1](src/gda_1.png)\n",
    "\n",
    "\n",
    "Repeat until convergence:\n",
    "$$\\theta_1 := \\theta_1 -\\alpha \\frac{d}{d \\theta_1} J(\\theta_1)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of the slope's sign for $\\frac{d}{d\\theta_1} J(\\theta_1)$, $\\theta_1$ eventually converges to its minimum value. The following graph shows that when the slope is negative, the value of $\\theta_1$ increases and when it is positive, the value of $\\theta_1$ decreases.\n",
    "\n",
    "![gda_2](src/gda_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a side note, we should adjust our parameter $\\alpha$ to ensure that the gradient descent algorithm converges in a reasonable time. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong.\n",
    "\n",
    "![gda_3](src/gda_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does gradient descent converge with a fixed step size $\\alpha$ ?\n",
    "\n",
    "The intuition behind the convergence is that $\\frac{d}{d\\theta_1} J(\\theta_1)$ approaches 0 as we approach the bottom of our convex function. At the minimum, the derivative will always be 0 and thus we get:\n",
    "\n",
    "![gda_4](src/gda_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz\n",
    "\n",
    "Suppose $\\theta_1$ is at a local optimum of $J(\\theta_1)$, such as shown in the figure.\n",
    "\n",
    "What will one step of gradient descent $\\theta_1 := \\theta_1 -\\alpha \\frac{d}{d \\theta_1} J(\\theta_1)$ do?\n",
    "\n",
    "![GDI Quiz](src/gdi_quiz.jpg)\n",
    "\n",
    "- [x] Leave $\\theta_1$ unchanged\n",
    "- [ ] Change $\\theta_1$ in a random direction\n",
    "- [ ] Move $\\theta_1$ in the direction of the global minimum of $J(\\theta_1)$\n",
    "- [ ] Decrease $\\theta_1$\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
